diff --git a/src/main/java/io/confluent/connect/jdbc/source/BulkTableQuerier.java b/src/main/java/io/confluent/connect/jdbc/source/BulkTableQuerier.java
index 3151637..267a851 100644
--- a/src/main/java/io/confluent/connect/jdbc/source/BulkTableQuerier.java
+++ b/src/main/java/io/confluent/connect/jdbc/source/BulkTableQuerier.java
@@ -16,6 +16,7 @@
 
 package io.confluent.connect.jdbc.source;
 
+import org.apache.kafka.connect.data.Schema.Type;
 import org.apache.kafka.connect.data.Struct;
 import org.apache.kafka.connect.errors.ConnectException;
 import org.apache.kafka.connect.source.SourceRecord;
@@ -36,8 +37,8 @@ import io.confluent.connect.jdbc.util.JdbcUtils;
 public class BulkTableQuerier extends TableQuerier {
   private static final Logger log = LoggerFactory.getLogger(BulkTableQuerier.class);
 
-  public BulkTableQuerier(QueryMode mode, String name, String schemaPattern, String topicPrefix) {
-    super(mode, name, topicPrefix, schemaPattern);
+  public BulkTableQuerier(QueryMode mode, String name, String schemaPattern, String topicPrefix, Map<String, Type> fieldTypeMap) {
+    super(mode, name, topicPrefix, schemaPattern, fieldTypeMap);
   }
 
   @Override
diff --git a/src/main/java/io/confluent/connect/jdbc/source/DataConverter.java b/src/main/java/io/confluent/connect/jdbc/source/DataConverter.java
index 2b717f8..ef281fd 100644
--- a/src/main/java/io/confluent/connect/jdbc/source/DataConverter.java
+++ b/src/main/java/io/confluent/connect/jdbc/source/DataConverter.java
@@ -19,6 +19,8 @@ package io.confluent.connect.jdbc.source;
 import org.apache.kafka.connect.data.Date;
 import org.apache.kafka.connect.data.Decimal;
 import org.apache.kafka.connect.data.Schema;
+import org.apache.kafka.connect.data.Schema.Type;
+import org.apache.kafka.connect.data.Field;
 import org.apache.kafka.connect.data.SchemaBuilder;
 import org.apache.kafka.connect.data.Struct;
 import org.apache.kafka.connect.data.Time;
@@ -38,7 +40,8 @@ import java.sql.Types;
 import java.util.Calendar;
 import java.util.GregorianCalendar;
 import java.util.TimeZone;
-
+import java.util.EnumSet;
+import java.util.Map;
 
 /**
  * DataConverter handles translating table schemas to Kafka Connect schemas and row data to Kafka
@@ -46,6 +49,9 @@ import java.util.TimeZone;
  */
 public class DataConverter {
   private static final Logger log = LoggerFactory.getLogger(JdbcSourceTask.class);
+  static final EnumSet<Type> SUPPORTED_EXPLICIT_TYPES = EnumSet.of(Type.BOOLEAN,
+      Type.INT8, Type.INT16, Type.INT32, Type.INT64, Type.FLOAT32, Type.FLOAT64, Type.STRING, Type.BYTES);
+
 
   private static final ThreadLocal<Calendar> UTC_CALENDAR = new ThreadLocal<Calendar>() {
     @Override
@@ -54,12 +60,12 @@ public class DataConverter {
     }
   };
 
-  public static Schema convertSchema(String tableName, ResultSetMetaData metadata)
+  public static Schema convertSchema(String tableName, ResultSetMetaData metadata, Map<String, Type> fieldTypeMap)
       throws SQLException {
     // TODO: Detect changes to metadata, which will require schema updates
     SchemaBuilder builder = SchemaBuilder.struct().name(tableName);
-    for (int col = 1; col <= metadata.getColumnCount(); col++) {
-      addFieldSchema(metadata, col, builder);
+    for (int col = 1, count = metadata.getColumnCount(); col <= count; col++) {
+      addFieldSchema(metadata, col, builder, fieldTypeMap);
     }
     return builder.build();
   }
@@ -70,8 +76,7 @@ public class DataConverter {
     Struct struct = new Struct(schema);
     for (int col = 1; col <= metadata.getColumnCount(); col++) {
       try {
-        convertFieldValue(resultSet, col, metadata.getColumnType(col), struct,
-                          metadata.getColumnLabel(col));
+        convertFieldValue(resultSet, col, struct, metadata);
       } catch (IOException e) {
         log.warn("Ignoring record because processing failed:", e);
       } catch (SQLException e) {
@@ -83,20 +88,33 @@ public class DataConverter {
 
 
   private static void addFieldSchema(ResultSetMetaData metadata, int col,
-                                     SchemaBuilder builder)
+                                     SchemaBuilder builder, Map<String, Type> fieldTypeMap)
       throws SQLException {
     // Label is what the query requested the column name be using an "AS" clause, name is the
     // original
     String label = metadata.getColumnLabel(col);
     String name = metadata.getColumnName(col);
-    String fieldName = label != null && !label.isEmpty() ? label : name;
+    String fieldName = (label != null && !label.isEmpty()) ? label : name;
 
-    int sqlType = metadata.getColumnType(col);
     boolean optional = false;
     if (metadata.isNullable(col) == ResultSetMetaData.columnNullable ||
         metadata.isNullable(col) == ResultSetMetaData.columnNullableUnknown) {
       optional = true;
     }
+
+    Type type = (fieldTypeMap != null) ? fieldTypeMap.get(fieldName) : null;
+    if (type != null) { // type is explicitly specified
+      enforceSchemaType(builder, fieldName, type, optional);
+    } else {
+      lookupSchemaType(metadata, col, builder, fieldName, optional);
+    }
+  }
+
+  private static void lookupSchemaType(ResultSetMetaData metadata, int col, SchemaBuilder builder,
+                                       String fieldName, boolean optional)
+      throws SQLException {
+
+    int sqlType = metadata.getColumnType(col);
     switch (sqlType) {
       case Types.NULL: {
         log.warn("JDBC type {} not currently supported", sqlType);
@@ -262,175 +280,160 @@ public class DataConverter {
     }
   }
 
-  private static void convertFieldValue(ResultSet resultSet, int col, int colType,
-                                        Struct struct, String fieldName)
-      throws SQLException, IOException {
-    final Object colValue;
-    switch (colType) {
-      case Types.NULL: {
-        colValue = null;
+  private static void enforceSchemaType(SchemaBuilder builder, String fieldName, Type type, boolean optional) {
+    switch (type) {
+      case BOOLEAN:
+        builder.field(fieldName, optional ? Schema.OPTIONAL_BOOLEAN_SCHEMA : Schema.BOOLEAN_SCHEMA);
         break;
-      }
 
-      case Types.BOOLEAN: {
-        colValue = resultSet.getBoolean(col);
+      case INT8:
+        builder.field(fieldName, optional ? Schema.OPTIONAL_INT8_SCHEMA : Schema.INT8_SCHEMA);
         break;
-      }
 
-      case Types.BIT: {
-        /**
-         * BIT should be either 0 or 1.
-         * TODO: Postgres handles this differently, returning a string "t" or "f". See the
-         * elasticsearch-jdbc plugin for an example of how this is handled
-         */
-        colValue = resultSet.getByte(col);
+      case INT16:
+        builder.field(fieldName, optional ? Schema.OPTIONAL_INT16_SCHEMA : Schema.INT16_SCHEMA);
         break;
-      }
 
-      // 8 bits int
-      case Types.TINYINT: {
-        colValue = resultSet.getByte(col);
+      case INT32:
+        builder.field(fieldName, optional ? Schema.OPTIONAL_INT32_SCHEMA : Schema.INT32_SCHEMA);
         break;
-      }
 
-      // 16 bits int
-      case Types.SMALLINT: {
-        colValue = resultSet.getShort(col);
+      case INT64:
+        builder.field(fieldName, optional ? Schema.OPTIONAL_INT64_SCHEMA : Schema.INT64_SCHEMA);
         break;
-      }
 
-      // 32 bits int
-      case Types.INTEGER: {
-        colValue = resultSet.getInt(col);
+      case FLOAT32:
+        builder.field(fieldName, optional ? Schema.OPTIONAL_FLOAT32_SCHEMA : Schema.FLOAT32_SCHEMA);
         break;
-      }
 
-      // 64 bits int
-      case Types.BIGINT: {
-        colValue = resultSet.getLong(col);
+      case FLOAT64:
+        builder.field(fieldName, optional ? Schema.OPTIONAL_FLOAT64_SCHEMA : Schema.FLOAT64_SCHEMA);
         break;
-      }
 
-      // REAL is a single precision floating point value, i.e. a Java float
-      case Types.REAL: {
-        colValue = resultSet.getFloat(col);
+      case STRING:
+        builder.field(fieldName, optional ? Schema.OPTIONAL_STRING_SCHEMA : Schema.STRING_SCHEMA);
         break;
-      }
 
-      // FLOAT is, confusingly, double precision and effectively the same as DOUBLE. See REAL
-      // for single precision
-      case Types.FLOAT:
-      case Types.DOUBLE: {
-        colValue = resultSet.getDouble(col);
+      case BYTES:
+        builder.field(fieldName, optional ? Schema.OPTIONAL_BYTES_SCHEMA : Schema.BYTES_SCHEMA);
         break;
-      }
 
-      case Types.NUMERIC:
-      case Types.DECIMAL: {
-        colValue = resultSet.getBigDecimal(col);
-        break;
-      }
+      default:
+        throw new IllegalArgumentException("Cannot enforce result schema type: " + type);
+    }
+  }
 
-      case Types.CHAR:
-      case Types.VARCHAR:
-      case Types.LONGVARCHAR: {
-        colValue = resultSet.getString(col);
-        break;
-      }
+  private static void convertFieldValue(ResultSet resultSet, int col,
+                                        Struct struct, ResultSetMetaData metadata) throws SQLException, IOException {
+    String fieldName = metadata.getColumnLabel(col);
+    Field field = struct.schema().field(fieldName);
+    final Object colValue = extractValue(resultSet, col, metadata, field.schema().type());
+    struct.put(field, resultSet.wasNull() ? null : colValue);
+  }
 
-      case Types.NCHAR:
-      case Types.NVARCHAR:
-      case Types.LONGNVARCHAR: {
-        colValue = resultSet.getNString(col);
-        break;
-      }
+  private static Object extractValue(ResultSet resultSet, int col,
+                                     ResultSetMetaData metadata, Type type) throws SQLException, IOException {
 
-      // Binary == fixed, VARBINARY and LONGVARBINARY == bytes
-      case Types.BINARY:
-      case Types.VARBINARY:
-      case Types.LONGVARBINARY: {
-        colValue = resultSet.getBytes(col);
-        break;
-      }
+    final Object colValue;
+    int sqlType = metadata.getColumnType(col);
 
-      // Date is day + moth + year
-      case Types.DATE: {
-        colValue = resultSet.getDate(col, UTC_CALENDAR.get());
-        break;
-      }
+    switch (type) {
+      case BOOLEAN:
+        return resultSet.getBoolean(col);
 
-      // Time is a time of day -- hour, minute, seconds, nanoseconds
-      case Types.TIME: {
-        colValue = resultSet.getTime(col, UTC_CALENDAR.get());
-        break;
-      }
+      case INT8:
+        return resultSet.getByte(col);
 
-      // Timestamp is a date + time
-      case Types.TIMESTAMP: {
-        colValue = resultSet.getTimestamp(col, UTC_CALENDAR.get());
-        break;
-      }
+      case INT16:
+        return resultSet.getShort(col);
 
-      // Datalink is basically a URL -> string
-      case Types.DATALINK: {
-        URL url = resultSet.getURL(col);
-        colValue = (url != null ? url.toString() : null);
-        break;
-      }
+      case INT32:
+        switch (sqlType) {
+          case Types.DATE:
+            return resultSet.getDate(col, UTC_CALENDAR.get());
 
-      // BLOB == fixed
-      case Types.BLOB: {
-        Blob blob = resultSet.getBlob(col);
-        if (blob == null) {
-          colValue = null;
-        } else {
-          if (blob.length() > Integer.MAX_VALUE) {
-            throw new IOException("Can't process BLOBs longer than Integer.MAX_VALUE");
-          }
-          colValue = blob.getBytes(1, (int) blob.length());
-          blob.free();
+          case Types.TIME:
+            return resultSet.getTime(col, UTC_CALENDAR.get());
+ 
+          default:
+            return resultSet.getInt(col);
         }
-        break;
-      }
-      case Types.CLOB:
-      case Types.NCLOB: {
-        Clob clob = (colType == Types.CLOB ? resultSet.getClob(col) : resultSet.getNClob(col));
-        if (clob == null) {
-          colValue = null;
-        } else {
-          if (clob.length() > Integer.MAX_VALUE) {
-            throw new IOException("Can't process BLOBs longer than Integer.MAX_VALUE");
-          }
-          colValue = clob.getSubString(1, (int) clob.length());
-          clob.free();
+ 
+      case INT64:
+        switch (sqlType) {
+          case Types.TIMESTAMP:
+            return resultSet.getTimestamp(col, UTC_CALENDAR.get());
+ 
+          default:
+            return resultSet.getLong(col);
+        }
+ 
+      case FLOAT32:
+        return resultSet.getFloat(col);
+
+      case FLOAT64:
+        return resultSet.getDouble(col);
+
+      case STRING:
+        switch (sqlType) {
+          case Types.SQLXML: 
+            SQLXML sqlxml = resultSet.getSQLXML(col);
+            return (sqlxml != null) ? sqlxml.getString() : null;
+          
+          case Types.DATALINK:
+            URL url = resultSet.getURL(col);
+            return (url != null) ? url.toString() : null;
+
+          case Types.NCHAR:
+          case Types.NVARCHAR:
+          case Types.LONGNVARCHAR:
+            return resultSet.getNString(col);
+
+          case Types.CLOB:
+          case Types.NCLOB:
+            Clob clob = (sqlType == Types.CLOB ? resultSet.getClob(col) : resultSet.getNClob(col));
+            if (clob == null)
+              return null;
+            else {
+              if (clob.length() > Integer.MAX_VALUE)
+                throw new IOException("Can't process BLOBs longer than Integer.MAX_VALUE");
+              String s = clob.getSubString(1, (int) clob.length());
+              clob.free();
+              return s;
+            }
+
+          default:
+            return resultSet.getString(col);
+        }
+ 
+      case BYTES:
+        switch (sqlType) {
+          // BLOB == fixed
+          case Types.BLOB:
+            Blob blob = resultSet.getBlob(col);
+            if (blob == null)
+              return null;
+            else {
+              if (blob.length() > Integer.MAX_VALUE)
+                throw new IOException("Can't process BLOBs longer than Integer.MAX_VALUE");
+              Object b = blob.getBytes(1, (int) blob.length());
+              blob.free();
+              return b;
+            }
+
+          case Types.NUMERIC:
+          case Types.DECIMAL:
+            return resultSet.getBigDecimal(col);
+
+          default:
+            return resultSet.getBytes(col);
         }
-        break;
-      }
-
-      // XML -> string
-      case Types.SQLXML: {
-        SQLXML xml = resultSet.getSQLXML(col);
-        colValue = (xml != null ? xml.getString() : null);
-        break;
-      }
 
-      case Types.ARRAY:
-      case Types.JAVA_OBJECT:
-      case Types.OTHER:
-      case Types.DISTINCT:
-      case Types.STRUCT:
-      case Types.REF:
-      case Types.ROWID:
-      default: {
-        // These are not currently supported, but we don't want to log something for every single
-        // record we translate. There will already be errors logged for the schema translation
-        return;
-      }
+      default:
+          //These are not currently supported, but we don't want to log something for every single
+          //record we translate. There will already be errors logged for the schema translation
+        return null;
     }
-
-    // FIXME: Would passing in some extra info about the schema so we can get the Field by index
-    // be faster than setting this by name?
-    struct.put(fieldName, resultSet.wasNull() ? null : colValue);
   }
 
 }
diff --git a/src/main/java/io/confluent/connect/jdbc/source/JdbcSourceConnectorConfig.java b/src/main/java/io/confluent/connect/jdbc/source/JdbcSourceConnectorConfig.java
index 9015ccd..0b50dad 100644
--- a/src/main/java/io/confluent/connect/jdbc/source/JdbcSourceConnectorConfig.java
+++ b/src/main/java/io/confluent/connect/jdbc/source/JdbcSourceConnectorConfig.java
@@ -128,6 +128,11 @@ public class JdbcSourceConnectorConfig extends AbstractConfig {
   public static final String QUERY_DEFAULT = "";
   private static final String QUERY_DISPLAY = "Query";
 
+  public static final String SCHEMA_CONFIG = "schema";
+  private static final String SCHEMA_DOC =
+      "A comma-separated list of custom column-schema mapping to enforce from the result";
+  private static final String SCHEMA_DISPLAY = "Schema";
+
   public static final String TOPIC_PREFIX_CONFIG = "topic.prefix";
   private static final String TOPIC_PREFIX_DOC =
       "Prefix to prepend to table names to generate the name of the Kafka topic to publish data "
@@ -194,6 +199,7 @@ public class JdbcSourceConnectorConfig extends AbstractConfig {
         .define(VALIDATE_NON_NULL_CONFIG, Type.BOOLEAN, VALIDATE_NON_NULL_DEFAULT, Importance.LOW, VALIDATE_NON_NULL_DOC, MODE_GROUP, 4, Width.SHORT, VALIDATE_NON_NULL_DISPLAY,
                 MODE_DEPENDENTS_RECOMMENDER)
         .define(QUERY_CONFIG, Type.STRING, QUERY_DEFAULT, Importance.MEDIUM, QUERY_DOC, MODE_GROUP, 5, Width.SHORT, QUERY_DISPLAY)
+        .define(SCHEMA_CONFIG, Type.STRING, null, Importance.MEDIUM, SCHEMA_DOC, MODE_GROUP, 6, Width.LONG, SCHEMA_DISPLAY)
         .define(POLL_INTERVAL_MS_CONFIG, Type.INT, POLL_INTERVAL_MS_DEFAULT, Importance.HIGH, POLL_INTERVAL_MS_DOC, CONNECTOR_GROUP, 1, Width.SHORT, POLL_INTERVAL_MS_DISPLAY)
         .define(BATCH_MAX_ROWS_CONFIG, Type.INT, BATCH_MAX_ROWS_DEFAULT, Importance.LOW, BATCH_MAX_ROWS_DOC, CONNECTOR_GROUP, 2, Width.SHORT, BATCH_MAX_ROWS_DISPLAY)
         .define(TABLE_POLL_INTERVAL_MS_CONFIG, Type.LONG, TABLE_POLL_INTERVAL_MS_DEFAULT, Importance.LOW, TABLE_POLL_INTERVAL_MS_DOC, CONNECTOR_GROUP, 3, Width.SHORT, TABLE_POLL_INTERVAL_MS_DISPLAY)
diff --git a/src/main/java/io/confluent/connect/jdbc/source/JdbcSourceTask.java b/src/main/java/io/confluent/connect/jdbc/source/JdbcSourceTask.java
index 9d2c2b6..6065df8 100644
--- a/src/main/java/io/confluent/connect/jdbc/source/JdbcSourceTask.java
+++ b/src/main/java/io/confluent/connect/jdbc/source/JdbcSourceTask.java
@@ -22,6 +22,7 @@ import org.apache.kafka.common.utils.Time;
 import org.apache.kafka.connect.errors.ConnectException;
 import org.apache.kafka.connect.source.SourceRecord;
 import org.apache.kafka.connect.source.SourceTask;
+import org.apache.kafka.connect.data.Schema.Type;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -31,13 +32,18 @@ import java.util.ArrayList;
 import java.util.Collections;
 import java.util.List;
 import java.util.Map;
+import java.util.HashMap;
 import java.util.PriorityQueue;
 import java.util.concurrent.atomic.AtomicBoolean;
+import static java.util.Collections.emptyMap;
 
 import io.confluent.connect.jdbc.util.CachedConnectionProvider;
 import io.confluent.connect.jdbc.util.JdbcUtils;
 import io.confluent.connect.jdbc.util.Version;
 
+import static io.confluent.connect.jdbc.source.JdbcSourceConnectorConfig.SCHEMA_CONFIG;
+import static io.confluent.connect.jdbc.source.DataConverter.SUPPORTED_EXPLICIT_TYPES;
+
 /**
  * JdbcSourceTask is a Kafka Connect SourceTask implementation that reads from JDBC databases and
  * generates Kafka Connect records.
@@ -45,6 +51,14 @@ import io.confluent.connect.jdbc.util.Version;
 public class JdbcSourceTask extends SourceTask {
 
   private static final Logger log = LoggerFactory.getLogger(JdbcSourceTask.class);
+  private static final Map<String, Type> SUPPORTED_EXPLICIT_TYPES_MAP;
+  static {
+    SUPPORTED_EXPLICIT_TYPES_MAP = new HashMap<>(SUPPORTED_EXPLICIT_TYPES.size(), 1f);
+    for (Type supportedType : SUPPORTED_EXPLICIT_TYPES) {
+      SUPPORTED_EXPLICIT_TYPES_MAP.put(supportedType.name().toLowerCase(), supportedType);
+    }
+  }
+
 
   private Time time;
   private JdbcSourceTaskConfig config;
@@ -138,25 +152,45 @@ public class JdbcSourceTask extends SourceTask {
       }
       Map<String, Object> offset = offsets == null ? null : offsets.get(partition);
 
+      Map<String, Type> fieldTypeMap = createFieldTypeMap(config.getString(SCHEMA_CONFIG));
+
       String topicPrefix = config.getString(JdbcSourceTaskConfig.TOPIC_PREFIX_CONFIG);
 
       if (mode.equals(JdbcSourceTaskConfig.MODE_BULK)) {
-        tableQueue.add(new BulkTableQuerier(queryMode, tableOrQuery, schemaPattern, topicPrefix));
+        tableQueue.add(new BulkTableQuerier(queryMode, tableOrQuery, schemaPattern, topicPrefix, fieldTypeMap));
       } else if (mode.equals(JdbcSourceTaskConfig.MODE_INCREMENTING)) {
         tableQueue.add(new TimestampIncrementingTableQuerier(
-            queryMode, tableOrQuery, topicPrefix, null, incrementingColumn, offset, timestampDelayInterval, schemaPattern));
+            queryMode, tableOrQuery, topicPrefix, null, incrementingColumn, offset, timestampDelayInterval, schemaPattern, fieldTypeMap));
       } else if (mode.equals(JdbcSourceTaskConfig.MODE_TIMESTAMP)) {
         tableQueue.add(new TimestampIncrementingTableQuerier(
-            queryMode, tableOrQuery, topicPrefix, timestampColumn, null, offset, timestampDelayInterval, schemaPattern));
+            queryMode, tableOrQuery, topicPrefix, timestampColumn, null, offset, timestampDelayInterval, schemaPattern, fieldTypeMap));
       } else if (mode.endsWith(JdbcSourceTaskConfig.MODE_TIMESTAMP_INCREMENTING)) {
         tableQueue.add(new TimestampIncrementingTableQuerier(
-            queryMode, tableOrQuery, topicPrefix, timestampColumn, incrementingColumn, offset, timestampDelayInterval, schemaPattern));
+            queryMode, tableOrQuery, topicPrefix, timestampColumn, incrementingColumn, offset, timestampDelayInterval, schemaPattern, fieldTypeMap));
       }
     }
 
     stop = new AtomicBoolean(false);
   }
 
+  private static Map<String, Type> createFieldTypeMap(String schemaString) {
+    if (schemaString == null || (schemaString = schemaString.trim()).isEmpty()) {
+      return emptyMap();
+    }
+    String[] fieldTypes = schemaString.split(",");
+    Map<String, Type> schema = new HashMap<>(fieldTypes.length, 1f);
+    for (String fieldType : fieldTypes) {
+      String[] fieldAndType = fieldType.trim().split(":");
+      if (fieldAndType.length < 2) {
+        continue;
+      }
+      String fieldName = fieldAndType[0].trim();
+      String type = fieldAndType[1].trim().toLowerCase();
+      schema.put(fieldName, SUPPORTED_EXPLICIT_TYPES_MAP.get(type));
+    }
+    return schema;
+  }
+
   @Override
   public void stop() throws ConnectException {
     if (stop != null) {
diff --git a/src/main/java/io/confluent/connect/jdbc/source/TableQuerier.java b/src/main/java/io/confluent/connect/jdbc/source/TableQuerier.java
index 7b8a17c..a0a31d7 100644
--- a/src/main/java/io/confluent/connect/jdbc/source/TableQuerier.java
+++ b/src/main/java/io/confluent/connect/jdbc/source/TableQuerier.java
@@ -17,6 +17,7 @@
 package io.confluent.connect.jdbc.source;
 
 import org.apache.kafka.connect.data.Schema;
+import org.apache.kafka.connect.data.Schema.Type;
 import org.apache.kafka.connect.source.SourceRecord;
 
 import java.sql.Connection;
@@ -24,6 +25,8 @@ import java.sql.PreparedStatement;
 import java.sql.ResultSet;
 import java.sql.SQLException;
 
+import java.util.Map;
+
 /**
  * TableQuerier executes queries against a specific table. Implementations handle different types
  * of queries: periodic bulk loading, incremental loads using auto incrementing IDs, incremental
@@ -46,13 +49,15 @@ abstract class TableQuerier implements Comparable<TableQuerier> {
   protected PreparedStatement stmt;
   protected ResultSet resultSet;
   protected Schema schema;
+  private Map<String, Type> fieldTypeMap;
 
-  public TableQuerier(QueryMode mode, String nameOrQuery, String topicPrefix, String schemaPattern) {
+  public TableQuerier(QueryMode mode, String nameOrQuery, String topicPrefix, String schemaPattern, Map<String, Type> fieldTypeMap) {
     this.mode = mode;
     this.schemaPattern = schemaPattern;
     this.name = mode.equals(QueryMode.TABLE) ? nameOrQuery : null;
     this.query = mode.equals(QueryMode.QUERY) ? nameOrQuery : null;
     this.topicPrefix = topicPrefix;
+    this.fieldTypeMap = fieldTypeMap;
     this.lastUpdate = 0;
   }
 
@@ -78,7 +83,7 @@ abstract class TableQuerier implements Comparable<TableQuerier> {
     if (resultSet == null) {
       stmt = getOrCreatePreparedStatement(db);
       resultSet = executeQuery();
-      schema = DataConverter.convertSchema(name, resultSet.getMetaData());
+      schema = DataConverter.convertSchema(name, resultSet.getMetaData(), fieldTypeMap);
     }
   }
 
diff --git a/src/main/java/io/confluent/connect/jdbc/source/TimestampIncrementingTableQuerier.java b/src/main/java/io/confluent/connect/jdbc/source/TimestampIncrementingTableQuerier.java
index 77bfc39..e1c20c7 100644
--- a/src/main/java/io/confluent/connect/jdbc/source/TimestampIncrementingTableQuerier.java
+++ b/src/main/java/io/confluent/connect/jdbc/source/TimestampIncrementingTableQuerier.java
@@ -18,6 +18,7 @@ package io.confluent.connect.jdbc.source;
 
 import org.apache.kafka.connect.data.Decimal;
 import org.apache.kafka.connect.data.Schema;
+import org.apache.kafka.connect.data.Schema.Type;
 import org.apache.kafka.connect.data.Struct;
 import org.apache.kafka.connect.errors.ConnectException;
 import org.apache.kafka.connect.source.SourceRecord;
@@ -68,8 +69,8 @@ public class TimestampIncrementingTableQuerier extends TableQuerier {
   public TimestampIncrementingTableQuerier(QueryMode mode, String name, String topicPrefix,
                                            String timestampColumn, String incrementingColumn,
                                            Map<String, Object> offsetMap, Long timestampDelay,
-                                           String schemaPattern) {
-    super(mode, name, topicPrefix, schemaPattern);
+                                           String schemaPattern, Map<String, Type> fieldTypeMap) {
+    super(mode, name, topicPrefix, schemaPattern, fieldTypeMap);
     this.timestampColumn = timestampColumn;
     this.incrementingColumn = incrementingColumn;
     this.timestampDelay = timestampDelay;
diff --git a/src/test/java/io/confluent/connect/jdbc/source/TimestampIncrementingTableQuerierTest.java b/src/test/java/io/confluent/connect/jdbc/source/TimestampIncrementingTableQuerierTest.java
index e8cf5e7..1b58e47 100644
--- a/src/test/java/io/confluent/connect/jdbc/source/TimestampIncrementingTableQuerierTest.java
+++ b/src/test/java/io/confluent/connect/jdbc/source/TimestampIncrementingTableQuerierTest.java
@@ -70,7 +70,7 @@ public class TimestampIncrementingTableQuerierTest {
   }
 
   private TimestampIncrementingTableQuerier newQuerier() {
-    return new TimestampIncrementingTableQuerier(TableQuerier.QueryMode.TABLE, null, "", null, "id", Collections.<String, Object>emptyMap(), 0L, null);
+    return new TimestampIncrementingTableQuerier(TableQuerier.QueryMode.TABLE, null, "", null, "id", Collections.<String, Object>emptyMap(), 0L, null, null);
   }
 
 }
