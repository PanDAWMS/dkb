=======================================
* Detailed description of the datafow *
=======================================

*** NOTE: stages are to be named with three leading digits (XXX_StageName). ***

=====================
* Brief description *
=====================
Stage            Stage Name                     Stage Description 
Number   
********************************************************************************************
0.0              000_kafka                      Dataflow managing Kafka applications
0.9              009_oracleConnector            Get task metadata from ProdSys
1.0              010_glancePapers               Get papers with links to supporting documents from GLANCE
1.5              015_CDSDocuments               Get metadata for paper (and its supporting documents) from CDS
1.6              016_task2es                    Prepare task metadata for bulk upload to ES
1.8              018_PDFDownloader              Download PFD documents and upload them to HDFS
1.9              019_oracle2esFormat            Prepare data for bulk upload to ES
2.5              025_chicagoES                  Get additional metadata from Chicago ES
3.0              030_PDFAnalyzer                PDF Analyzer (search dataset in PDF Supporting Notes)
5.4              054_docContent2TTL             Convert results of Stage 30 to triples
5.5              055_docs2TTL                   Convert paper (and its supporting documents) metadata to triples
6.0              060_upload2virtuoso            Upload data to Virtuoso
6.9              069_upload2es                  Upload data to ES
9.1              091_datasetsRucio              Get dataset metadata from Rucio
9.3              093_datasetsFormat             Update dataset metadata:
                                                  add "data_format" field
9.5              095_datasetInfoAMI             Get dataset metadata from AMI



=========================
* Dataflows description *
=========================
Dataflow                                                        Description
********************************************************************************************************
010/output -> 015                                               using output of 010 as input for 015
015/output -> (018|055)
018/output -> 030/output -> 054                                 download PDF, analyze them and prepare results for Virtuoso
015/output -> 055                                               generate TTL file for paper and its supporting documents
                                                                  metadata
(054|055)/output -> 060                                         upload links, papers, support docs and datasets TTL
                                                                  files to Virtuoso

009/output ---> 091(in)/output ---> *                           Task metadata from Oracle to Elasticsearch
  \       * ---> 025/output ---> 016/output ---> **
   \                           ** ----> 019/output -> 069
    \                                         /
     \--> 091(out)/output ---> ***           /                  Dataset metadata from Rucio and AMI to Elasticsearch
         *** --> 093/output -> 095/output --/

==============
* REFERENCES *
==============
Conventions, in-between data location in HDFS storage etc.

===========================
* 1. Data Samples in HDFS *
===========================
Catalog                                 Description                                             Link to Dataflow Stage
***********************************************************************************************************************
/user/DKB/store/PDF                     PDF documents loaded from CDS                           [by 018_PDFDownloader]
/user/DKB/store/DatasetsFromPDF         dataset names and ID's, found by PDFAnalyzer module     [030_PDFAnalyzer/output]
/user/DKB/store/document-metadata       ATLAS documents metadata from CDS and GLANCE            [015_CDSDocuments/output]


=======================================
* 2. Streaming mode (data processing) *
=======================================
To automate all the processes in the DataFlow, we need to run all the stages in
a streaming mode.  It means, that every (processing) stage is to meet the
following requirements:
- can be run in a quasi-infinite loop (waiting for input data and reading and
  so on unless the input stream is closed) [1]; - read input data from STDIN;
- write output data to STDOUT;
- not to write anything but processing result to STDOUT (meaning that any log
  and error messages are going to STDERR or can be suppressed by some kind of
  --silent option); - in case of multiple output messages as a result of
  processing of one input message, use '\0' as a separator between sets of
  output messages [2];
- [not implemented] in case of multiline result use empty line or '\3'
  as a message separator[3].

[1] In case of input data that cannot be easily transformed in a 'one-line
    message' (as PDF files), use names of the files in HDFS as input or learn
    how to distinguish where one input message ends and starts another one.

[2] For now, ExternalProcessor supposes that even one-message results should be
    separated with '\0'.

[3] Not implemented yet in ExternalProcessor, so let M.Golosova know if it is a
    crying need.

=====================================
* 3. External sources (data mining) *
=====================================
There are two general cases of external data sources.
1. Standard sources (like RDBMS) - here we can use standard connectors.
2. Non-standard sources (like GLANCE).

For non-standard sources there are three ways to connect them with Kafka:
- custom Connector for every source (would be the most Kafka way);
- custom Connector, which runs external program and consumes its output (would
  be nice: still looks Kafka-way yet allows us to use external programs);
- run FileStreamSource Connector, feeding it with data from a named pipe - and
  write data to the pipe via external program.

The last variant looked as the simpliest way, and was implemented for tests.

Now the basic way is to use External Connector.
For the external program the main cycle looks this way:
- start
- get data from the external source
- output data to the STDOUT (one record per line)
- exit

If it is required to save the connector state, it is up to the external program
for now.
