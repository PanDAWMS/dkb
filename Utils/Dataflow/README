=======================================
* Detailed description of the dataflow *
=======================================

=======================
* General information *
=======================


1. Main concepts
----------------

"Dataflow" is a series of "Stages", applied to a stream of data in a specific
order. Each Stage is nothing but a program capable of operating in a "stream
mode". It means, basically, that the program can read input stream of
"Messages" and/or generate the output one according to a set of rules common
for all the stages.


2. Libraries
------------

./shell_lib/

-- common functions for Shell scripts. Currently used by scripts run/*.sh, but
   may also be useful for stage scripts. Usage example:

       lib_dir="$base_dir/../shell_lib"

       if ! [ -d "$lib_dir" ]; then
         echo "Library directory '$lib_dir' not found!"
         exit 1
       fi

       . "$lib_dir/log"

       log DEBUG "My debug message here."


./pyDKB/

-- Python (v.2.7) library for stage scripts. Contains implementation of basic
   operations on the data flow (read/write messages) for different versions of
   the flow (local files, HDFS files, STDIN/OUT, map/reduce), provides default
   parser for stage configuration files, etc. Most of this functionality is not
   necessary to make program to operate as a Stage -- but was found useful for
   reliability and uniformity.

   The library can be installed system-wide or in a virtual environment:

       # Install current version (will exist independently of source files)
       $ ./pyDKB-setup.py install

       # Install development version (will use local source files)
       $ ./pyDKB-setup.py develop

       # Install in a virtual environment
       $ virtualenv -p `which python2.7` dkb-dev-env
       $ . dkb-dev-env/bin/activate
       (dkb-dev-env)$ ./pyDKB-setup.py develop
       (dkb-dev-env)$ <...>
       (dkb-dev-env)$ deactivate
       $ <...>

   Usage example can be found in directory: ./skel/

   Documentation can be found at the top level of the repository in directory:
   /Docs/


3. Other directories
--------------------

./config/ -- examples of configuration files for stages.

./run/    -- dataflow process management files (start/stop/status/...).

./test/   -- misc tools for this or that functionality testing:
  pyDKB/    -- tests for main pyDKB usage scenatios;
  utils/    -- misc tools

./XXX_StageName/
          -- Dataflow stage (see below).
             Stages are named with three leading digits (XXX), usualy referred
             to as stage ID. But the number assignment logic is long gone, so
             it's safe to think about them simply as unique IDs.


======================
* Stages description *
======================

               Supervisor Stages
---------------------------------------------------
#       Name                           Description
---------------------------------------------------
0.0     000_kafka                      Dataflow managing Kafka applications


           Document analysis Stages
---------------------------------------------------
#       Name                           Description
---------------------------------------------------
1.0     010_glancePapers               Get papers with links to supporting
                                         documents from GLANCE
1.5     015_CDSDocuments               Get metadata for paper (and its
                                         supporting documents) from CDS
1.8     018_PDFDownloader              Download PDF documents and upload them
                                         to HDFS
3.0     030_PDFAnalyzer                PDF Analyzer (search dataset in PDF
                                         Supporting Notes)
5.4     054_docContent2TTL             Convert results of Stage 30 to triples
5.5     055_docs2TTL                   Convert paper (and its supporting
                                         documents) metadata to triples
6.0     060_upload2virtuoso            Upload data to Virtuoso


     Task and DS metadata integration Stages
---------------------------------------------------
#       Name                           Description
---------------------------------------------------
0.9     009_oracleConnector            Get task metadata from ProdSys
1.6     016_task2es                    Update task metadata with fields
                                         required for ES indexing
1.7     017_adjustMetadata             Transform task metadata to fit ES scheme
1.9     019_oracle2esFormat            Prepare data for bulk upload to ES
2.5     025_chicagoES                  Get additional metadata from Chicago ES
6.9     069_upload2es                  Upload data to ES
7.1     071_esConsistency              Check data in ES
9.1     091_datasetsRucio              Get dataset metadata from Rucio
9.3     093_datasetsFormat             Update dataset metadata: add
                                         "data_format" field
9.5     095_datasetInfoAMI             Get dataset metadata from AMI


=========================
* Dataflows description *
=========================


1. Document analysis
--------------------

010 --> 015 --> *                    | Get papers and supporting docs metadata
                                     |
    * -> 018 -> 030 -> 054           | Get supp. doc PDFs, analyze and
     \                  \            |   prepare extracted metadata
      \                  \           |   for upload to Virtuoso
       \                  \          |
        \----> 055 ----------> **    | Prepare papers and supp. docs
                                     |  metadata for upload to Virtuoso
                                     |
    ** ---> 060                      | Upload prepared metadata to Virtuoso


2. Tasks and data samples (DS) metadata integration (ProdSys2)
--------------------------------------------------------------

009 ---> *                                | Get task metadata from ProdSys2 DB
                                          |
  * -> 091(in) -> 025 -> 016 -> 017       | Append task metadata with that from
   \                             \        |   Rucio (input DS) and Chicago ES
    \                             \       |
     \-> 091(out) -> 093 -> 095 -----> ** | Get DS metadata from Rucio and AMI
                                          |
  ** ----> 019 -> 069                     | Prepare and upload metadata to ES


3. Tasks metadata consistency check
-----------------------------------

009 ---> 016 ---> 071

This is a simplified and slightly changed version of the previous dataflow,
intended for making sure that information is consistent between ProdSys2 and
ES. It gets a very basic set of metadata from ProdSys2, adds ES-related
fields, and checks that it is present in ES rather than uploading it.

==============
* REFERENCES *
==============
Conventions, in-between data location in HDFS storage etc.


1. Data Samples in HDFS
-----------------------

HOME folder: /user/DKB

Stage  HDFS                           Description
*******************************************************************
018    $HOME/store/PDF                PDF documents loaded from CDS
030    $HOME/store/DatasetsFromPDF    dataset names and ID's, found by stage 30
015    $HOME/store/document-metadata  ATLAS documents metadata from CDS/GLANCE


2. Streaming mode (data processing)
-----------------------------------

To automate all the processes in the DataFlow, we need to run all the stages in
a streaming mode. It means, that every (processing) stage is to meet the
following requirements:
- can be run in a quasi-infinite loop (waiting for input data and reading and
  so on unless the input stream is closed) [1]; - read input data from STDIN;
- write output data to STDOUT, separating records with EOM [2];
- not to write anything but processing result to STDOUT (meaning that any log
  and error messages are going to STDERR or can be suppressed by some kind of
  --silent option);
- indicate that input message processing is finished by sending EOP [3] to
  STDOUT (note that last output record must ends with EOM, as any other record);


[1] In case of input data that cannot be easily transformed in a 'one-line
    message' (as PDF files), use names of the files in HDFS as input or learn
    how to distinguish where one input message ends and starts another one.

[2] EOM, End-of-Message marker. Default value: '\n' (newline)

[3] EOP, End-of-Process marker. Default value: '\0' (NULL)


3. Stage types
--------------

There are three types of stages:

1. Source stage - an initial stage in a dataflow pipeline that gets data
from an external source (CDS, Rucio, AMI, etc.).

2. Transform stage - a transitional stage that performs single logical
operation on the metadata. Set (chain) of transform stages forms the whole
transformation process for messages produced by a source stage. Transformation
operations may include: format change, derived values calculation,
message extension with metadata from an additional source, etc.

2.1. Pre-sink stage - the last transform stage in a dataflow pipeline that
prepares metadata for upload via sink stage; it is aware of messages'
formatting rules. All the preparations for upload should be done by
the pre-sink stage.

3. Sink stage - a stage that uploads data into the DKB storage
(Virtuoso, ES, ...). Every single sink stage should be preceded by
a corresponding pre-sink stage that transforms data and forms ready-to-send
messages with a specific structure. Sink stage only gets prepared messages
and uploads them without any re-formatting. It does not care about messages
content. If message does not follow the template, most likely it
will be rejected by the storage load API.


4. External sources (data mining)
---------------------------------

There are two general cases of external data sources.
1. Standard sources (like RDBMS) - here we can use standard connectors.
2. Non-standard sources (like GLANCE).

For non-standard sources there are three ways to connect them with Kafka:
- custom Connector for every source (would be the most Kafka way);
- custom Connector, which runs external program and consumes its output (would
  be nice: still looks Kafka-way yet allows us to use external programs);
- run FileStreamSource Connector, feeding it with data from a named pipe - and
  write data to the pipe via external program.

The last variant looked as the simpliest way, and was implemented for tests.

Now the basic way is to use External Connector.
For the external program the main cycle looks this way:
- start
- get data from the external source
- output data to the STDOUT (one record per line)
- exit

If it is required to save the connector state, it is up to the external program
for now.
