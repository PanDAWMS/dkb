=======================================
* Detailed description of the datafow *
=======================================

*** NOTE: stages are to be named with three leading digits (XXX_StageName). ***

=====================
* Brief description *
=====================
Stage   Stage Name                     Stage Description
Number
*******************************************************************************
0.0     000_kafka                      Dataflow managing Kafka applications
0.9     009_oracleConnector            Get task metadata from ProdSys
1.0     010_glancePapers               Get papers with links to supporting
                                         documents from GLANCE
1.5     015_CDSDocuments               Get metadata for paper (and its
                                         supporting documents) from CDS
1.6     016_task2es                    Prepare task metadata for bulk upload
                                         to ES
1.8     018_PDFDownloader              Download PFD documents and upload them
                                         to HDFS
1.9     019_oracle2esFormat            Prepare data for bulk upload to ES
2.5     025_chicagoES                  Get additional metadata from Chicago ES
3.0     030_PDFAnalyzer                PDF Analyzer (search dataset in PDF
                                         Supporting Notes)
5.4     054_docContent2TTL             Convert results of Stage 30 to triples
5.5     055_docs2TTL                   Convert paper (and its supporting
                                         documents) metadata to triples
6.0     060_upload2virtuoso            Upload data to Virtuoso
6.9     069_upload2es                  Upload data to ES
9.1     091_datasetsRucio              Get dataset metadata from Rucio
9.3     093_datasetsFormat             Update dataset metadata: add
                                         "data_format" field
9.5     095_datasetInfoAMI             Get dataset metadata from AMI



=========================
* Dataflows description *
=========================

1. Document analysis
---

010 --> 015 --> *                    | Get papers and supporting docs metadata
                                     |
    * -> 018 -> 030 -> 054           | Get supp. doc PDFs, analyze and
     \                  \            |   prepare extracted metadata
      \                  \           |   for upload to Virtuoso
       \                  \          |
        \----> 055 ----------> **    | Prepare papers and supp. docs
                                     |  metadata for upload to Virtuoso
                                     |
    ** ---> 060                      | Upload prepared metadata to Virtuoso

2. Tasks and data samples (DS) metadata integration (ProdSys2)
---

009 ---> *                                | Get task metadata from ProdSys2 DB
                                          |
    * --> 091(in) --> 025 --> 016         | Append task metadata with that from
     \                           \        |   Rucio (input DS) and Chicago ES
      \                           \       |
       \-> 091(out) -> 093 -> 095 --> **  | Get DS metadata from Rucio and AMI
                                          |
    ** ----> 019 -> 069                   | Prepare and upload metadata to ES

==============
* REFERENCES *
==============
Conventions, in-between data location in HDFS storage etc.

===========================
* 1. Data Samples in HDFS *
===========================
Catalog                                 Description                                             Link to Dataflow Stage
***********************************************************************************************************************
/user/DKB/store/PDF                     PDF documents loaded from CDS                           [by 018_PDFDownloader]
/user/DKB/store/DatasetsFromPDF         dataset names and ID's, found by PDFAnalyzer module     [030_PDFAnalyzer/output]
/user/DKB/store/document-metadata       ATLAS documents metadata from CDS and GLANCE            [015_CDSDocuments/output]


=======================================
* 2. Streaming mode (data processing) *
=======================================
To automate all the processes in the DataFlow, we need to run all the stages in
a streaming mode.  It means, that every (processing) stage is to meet the
following requirements:
- can be run in a quasi-infinite loop (waiting for input data and reading and
  so on unless the input stream is closed) [1]; - read input data from STDIN;
- write output data to STDOUT;
- not to write anything but processing result to STDOUT (meaning that any log
  and error messages are going to STDERR or can be suppressed by some kind of
  --silent option); - in case of multiple output messages as a result of
  processing of one input message, use '\0' as a separator between sets of
  output messages [2];
- [not implemented] in case of multiline result use empty line or '\3'
  as a message separator[3].

[1] In case of input data that cannot be easily transformed in a 'one-line
    message' (as PDF files), use names of the files in HDFS as input or learn
    how to distinguish where one input message ends and starts another one.

[2] For now, ExternalProcessor supposes that even one-message results should be
    separated with '\0'.

[3] Not implemented yet in ExternalProcessor, so let M.Golosova know if it is a
    crying need.

=====================================
* 3. External sources (data mining) *
=====================================
There are two general cases of external data sources.
1. Standard sources (like RDBMS) - here we can use standard connectors.
2. Non-standard sources (like GLANCE).

For non-standard sources there are three ways to connect them with Kafka:
- custom Connector for every source (would be the most Kafka way);
- custom Connector, which runs external program and consumes its output (would
  be nice: still looks Kafka-way yet allows us to use external programs);
- run FileStreamSource Connector, feeding it with data from a named pipe - and
  write data to the pipe via external program.

The last variant looked as the simpliest way, and was implemented for tests.

Now the basic way is to use External Connector.
For the external program the main cycle looks this way:
- start
- get data from the external source
- output data to the STDOUT (one record per line)
- exit

If it is required to save the connector state, it is up to the external program
for now.
