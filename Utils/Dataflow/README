=======================================
* Detailed description of the datafow *
=======================================

*** NOTE: stages are to be named with three leading digits (XXX_StageName). ***

======================
* Stages description *
======================

               Supervisor Stages
---------------------------------------------------
#       Name                           Description
---------------------------------------------------
0.0     000_kafka                      Dataflow managing Kafka applications


           Document analysis Stages
---------------------------------------------------
#       Name                           Description
---------------------------------------------------
1.0     010_glancePapers               Get papers with links to supporting
                                         documents from GLANCE
1.5     015_CDSDocuments               Get metadata for paper (and its
                                         supporting documents) from CDS
1.8     018_PDFDownloader              Download PFD documents and upload them
                                         to HDFS
3.0     030_PDFAnalyzer                PDF Analyzer (search dataset in PDF
                                         Supporting Notes)
5.4     054_docContent2TTL             Convert results of Stage 30 to triples
5.5     055_docs2TTL                   Convert paper (and its supporting
                                         documents) metadata to triples
6.0     060_upload2virtuoso            Upload data to Virtuoso


     Task and DS metadata integration Stages
---------------------------------------------------
#       Name                           Description
---------------------------------------------------
0.9     009_oracleConnector            Get task metadata from ProdSys
1.6     016_task2es                    Update task metadata with fields
                                         required for ES indexing
1.7     017_adjustMetadata             Transform task metadata to fit ES scheme
1.9     019_oracle2esFormat            Prepare data for bulk upload to ES
2.5     025_chicagoES                  Get additional metadata from Chicago ES
6.9     069_upload2es                  Upload data to ES
7.1     071_esConsistency              Check data in ES
9.1     091_datasetsRucio              Get dataset metadata from Rucio
9.3     093_datasetsFormat             Update dataset metadata: add
                                         "data_format" field
9.5     095_datasetInfoAMI             Get dataset metadata from AMI


=========================
* Dataflows description *
=========================

1. Document analysis
---

010 --> 015 --> *                    | Get papers and supporting docs metadata
                                     |
    * -> 018 -> 030 -> 054           | Get supp. doc PDFs, analyze and
     \                  \            |   prepare extracted metadata
      \                  \           |   for upload to Virtuoso
       \                  \          |
        \----> 055 ----------> **    | Prepare papers and supp. docs
                                     |  metadata for upload to Virtuoso
                                     |
    ** ---> 060                      | Upload prepared metadata to Virtuoso

2. Tasks and data samples (DS) metadata integration (ProdSys2)
---

009 ---> *                                | Get task metadata from ProdSys2 DB
                                          |
  * -> 091(in) -> 025 -> 016 -> 017       | Append task metadata with that from
   \                             \        |   Rucio (input DS) and Chicago ES
    \                             \       |
     \-> 091(out) -> 093 -> 095 -----> ** | Get DS metadata from Rucio and AMI
                                          |
  ** ----> 019 -> 069                     | Prepare and upload metadata to ES

3. Tasks metadata consistency check
---

009 ---> 016 ---> 071

This is a simplified and slightly changed version of the previous dataflow,
intended for making sure that information is consistent between ProdSys2 and
ES. It gets a very basic set of metadata from ProdSys2, adds ES-related
fields, and checks that it is present in ES rather than uploading it.

==============
* REFERENCES *
==============
Conventions, in-between data location in HDFS storage etc.

===========================
* 1. Data Samples in HDFS *
===========================

HOME folder: /user/DKB

Stage  HDFS                           Description
*******************************************************************
018    $HOME/store/PDF                PDF documents loaded from CDS
030    $HOME/store/DatasetsFromPDF    dataset names and ID's, found by stage 30
015    $HOME/store/document-metadata  ATLAS documents metadata from CDS/GLANCE


=======================================
* 2. Streaming mode (data processing) *
=======================================
To automate all the processes in the DataFlow, we need to run all the stages in
a streaming mode.  It means, that every (processing) stage is to meet the
following requirements:
- can be run in a quasi-infinite loop (waiting for input data and reading and
  so on unless the input stream is closed) [1]; - read input data from STDIN;
- write output data to STDOUT, separating records with EOM [2];
- not to write anything but processing result to STDOUT (meaning that any log
  and error messages are going to STDERR or can be suppressed by some kind of
  --silent option);
- indicate that input message processing is finished by sending EOP [3] to
  STDOUT (note that last output record must ends with EOM, as any other record);


[1] In case of input data that cannot be easily transformed in a 'one-line
    message' (as PDF files), use names of the files in HDFS as input or learn
    how to distinguish where one input message ends and starts another one.

[2] EOM, End-of-Message marker. Default value: '\n' (newline)

[3] EOP, End-of-Process marker. Default value: '\0' (NULL)

==================
* 3. Stage types *
==================
There are three types of stages:

1. Source stage - an initial stage in a dataflow pipeline that gets data
from an external source (CDS, Rucio, AMI, etc.).

2. Transform stage - these stages perform a set of operations for different
cases on metadata and form output messages according to a particular pattern
(e.g. select relevant metadata from a raw dataset, append metadata
to a message, prepare messages for upload).

2.1. Pre-sink stage - the last transform stage in a dataflow pipeline that
prepares metadata for upload via sink stage; it awares about messages
formatting rules. All the preparations should be done by the pre-sink
stage.

3. Sink stage - a stage that uploads data into the DKB storage
(Virtuoso, ES, ...). Every single sink stage should be preceded by
a corresponding pre-sink stage that transforms data and forms ready-to-send
messages with a specific structure. Sink stage only gets prepared messages
and uploads it without any re-formatting. It does not care about messages
content. If message does not follow the template, most likely it
will be rejected by the storage load API.

=====================================
* 4. External sources (data mining) *
=====================================
There are two general cases of external data sources.
1. Standard sources (like RDBMS) - here we can use standard connectors.
2. Non-standard sources (like GLANCE).

For non-standard sources there are three ways to connect them with Kafka:
- custom Connector for every source (would be the most Kafka way);
- custom Connector, which runs external program and consumes its output (would
  be nice: still looks Kafka-way yet allows us to use external programs);
- run FileStreamSource Connector, feeding it with data from a named pipe - and
  write data to the pipe via external program.

The last variant looked as the simpliest way, and was implemented for tests.

Now the basic way is to use External Connector.
For the external program the main cycle looks this way:
- start
- get data from the external source
- output data to the STDOUT (one record per line)
- exit

If it is required to save the connector state, it is up to the external program
for now.
