#!/bin/bash -l

SCRIPT_NAME=data4es

BATCH_SIZE=100
DEBUG=

base_dir=$( cd "$(dirname "$(readlink -f "$0")")"; pwd)
lib="$base_dir/../shell_lib"

source $lib/log

# Directories with configuration files
[ -n "$DATA4ES_CONFIG_PATH" ] && \
    CONFIG_PATH="$DATA4ES_CONFIG_PATH" || \
    CONFIG_PATH="${base_dir}/../config:${base_dir}/../../Elasticsearch/config"

# Process home dir
[ -n "$DATA4ES_HOME" ] && \
    HOME_DIR="$DATA4ES_HOME" || \
    HOME_DIR="${base_dir}/.data4es"

# Check if process is running
DATA4ES_HOME=$HOME_DIR ${base_dir}/data4es-status &>/dev/null
[ $? -eq 0 ] && log WARN "Process already running." && exit 1

# Create / clean home directory
AUX_FILES="pid"
mkdir -p "$HOME_DIR"
for f in $AUX_FILES; do
  rm -rf "$HOME_DIR/$f"
done

# Create aux directories and files
# ---
pipe_dir=$HOME_DIR/.pipe
mkdir -p $pipe_dir

buffer_dir=$HOME_DIR/.buffer
mkdir -p $buffer_dir

pidfile="$HOME_DIR/pid"

# Define commands to be used as dataflow nodes
# ---

source $lib/get_config

# Oracle Connector
cmd_09="${base_dir}/../009_oracleConnector/Oracle2JSON.py"
cfg_09=`get_config "009.cfg"`
cmd_OC="$cmd_09 --config $cfg_09"

# Stage 25
cmd_25="${base_dir}/../025_chicagoES/stage.py -m s"

# Stage 16
cmd_16="${base_dir}/../016_task2es/task2es.py -m s"

# Stage 19
cfg_19=`get_config "es"`
cmd_19="${base_dir}/../019_esFormat/run.sh --config $cfg_19"

# Stage 69
cfg_69=`get_config "es"`
cmd_69="${base_dir}/../069_upload2es/load_data.sh --config $cfg_69"

# Stage 91 (input)
cmd_91_in="${base_dir}/../091_datasetsRucio/datasets_processing.py -m s -t i"

# Stage 91 (output)
cmd_91_out="${base_dir}/../091_datasetsRucio/datasets_processing.py -m s"

# Stage 93
cmd_93="${base_dir}/../093_datasetsFormat/datasets_format.py -m s"

# Stage 95
auth_cfg=`get_config "095.cfg"`
source $auth_cfg
[ -r "$AUTH_KEY" -a -r "$AUTH_CRT" ] \
  || { log ERROR "Stage 095: wrong authentication parameters (failed to read certificate files: $AUTH_KEY and $AUTH_CRT)";
       exit 1; }
cmd_95="${base_dir}/../095_datasetInfoAMI/amiDatasets.py -m s --userkey $AUTH_KEY --usercert $AUTH_CRT"
# ---

# Service (glue) functions
# ---
source $lib/eop_filter

# Buffer file name
get_buffer() {
  [ -n "$1" ] && name=$1 || name=default
  echo $buffer_dir/$name
}

# Flush Sink Connector buffer to STDOUT and reset it
flush_buffer() {
  buffer=`get_buffer "$1"`
  if [ -r "$buffer" ]; then
    cat $buffer
    echo -e '\x00'
    rm -f $buffer
  fi
}

# Glue between processing functions and Sink Connector
# Buffers records and then send them as batch of $BATCH_SIZE
mediator() {
  buffer=`get_buffer "$1"`
  i=0
  while read -r line; do
    echo $line >> $buffer
    let i=$i+1
    let f=$i%$BATCH_SIZE
    [ $f -eq 0 ] && flush_buffer "$1"
  done
  flush_buffer "$1"
}
# ---

# Create named pipes for dataflow branching
# ---
branch() {
  [ -z "$1" ] \
    && log ERROR "branch(): Branch name not specified."\
    && return 1

  name="$pipe_dir/$1"
  if [ -e "$name" ]; then
    [ -p "$name" ] \
      && { echo $name; return 0; } \
      || name=`mktemp -u $name.XXXX`
  fi

  mkfifo $name \
    || { log ERROR "branch(): Failed to create named pipe for branch $1";
         return 1; }

  echo $name
}

b_91=`branch b_91` && b_16=`branch b_16` \
  || exit $?
# ---

# Run Dataflow
# ---

# Parse command line parameters
while [ -n "$1" ]; do
  case $1 in
    --debug)
      DEBUG=1;;
    --)
      shift
      break;;
    -*)
      log ERROR "Unknown option: $1";;
    *)
      break;;
  esac
  shift
done


# Define subchains
# Source subchain: Oracle Connector
source_chain() {
  $cmd_OC | tee $b_16 $b_91
}

# Parallel subchains: 16 and 91
chain_16() {
  [ -n DEBUG ] \
    && out_91='91_in.out' \
    && out_25='25.out'
  cat $b_16 | $cmd_91_in | eop_filter | tee $out_91 \
            | $cmd_25 | eop_filter | tee $out_25 \
            | $cmd_16 | eop_filter
}

chain_91() {
  out=/dev/null
  [ -n "$DEBUG" ] \
    && out_91='91_out.out' \
    && out_93='93.out'
  cat $b_91 | $cmd_91_out | eop_filter | tee $out_91 \
            | $cmd_93 | eop_filter | tee $out_93 \
            | $cmd_95 | eop_filter
}

# Sink chain
sink_chain() {
  [ -n "$DEBUG" ] \
    && cmd="tee 69.$1.inp" \
    || cmd=$cmd_69
  $cmd_19 | eop_filter | mediator "$1" | $cmd > /dev/null
}

log "Starting process."
t1=`date +%s`

out=/dev/null
[ -n "$DEBUG" ] \
  && out='src.out'
source_chain > $out &

[ -n "$DEBUG" ] \
  && out='16.out'
chain_16 | tee $out | sink_chain b16 &

[ -n "$DEBUG" ] \
  && out='95.out'
chain_91 | tee $out | sink_chain b91 &

ps ax | grep "$cmd_OC" | grep -v grep | awk '{print $1 " source"}' >> $pidfile
for cmd in "$cmd_25" "$cmd_16" "$cmd_91_in" "$cmd_91_out" "$cmd_93" \
           "$cmd_95" "$cmd_19" "$cmd_69"; do
  ps ax | grep "$cmd" | awk '{print $1}' >> $pidfile
done

wait

t2=`date +%s`
let "took=$t2-$t1"
log "Finished process (took: $took sec)." >&2
