#!/bin/bash -l

START_OFFSET="09-05-2016 00:00:00"
BATCH_SIZE=100
DEBUG=

base_dir=$( cd "$(dirname "$(readlink -f "$0")")"; pwd)

# Directories with configuration files
[ -n "$DATA4ES_CONFIG_PATH" ] && \
    CONFIG_PATH="$DATA4ES_CONFIG_PATH" || \
    CONFIG_PATH="${base_dir}/../config:${base_dir}/../../Elasticsearch/config"

# Process home dir
[ -n "$DATA4ES_HOME" ] && \
    HOME_DIR="$DATA4ES_HOME" || \
    HOME_DIR="${base_dir}/.data4es"

# Check if process is running
DATA4ES_HOME=$HOME_DIR ${base_dir}/data4es-status &>/dev/null
[ $? -eq 0 ] && echo "Process already running." >&2 && exit 1

# Create / clean home directory
AUX_FILES="pid"
mkdir -p "$HOME_DIR"
for f in $AUX_FILES; do
  rm -rf "$HOME_DIR/$f"
done

# Create aux directories and files
# ---
pipe_dir=$HOME_DIR/.pipe
mkdir -p $pipe_dir

buffer_dir=$HOME_DIR/.buffer
mkdir -p $buffer_dir

pidfile="$HOME_DIR/pid"

# Define commands to be used as dataflow nodes
# ---

# Find configuration file in $CONFIG_PATH
get_config() {
  [ -z "$1" ] && echo "get_config(): no argumets passed." && return 1
  dirs=$CONFIG_PATH
  while [ -n "$dirs" ]; do
    dir=${dirs%%:*}
    [ "$dirs" = "$dir" ] && \
        dirs='' || \
        dirs="${dirs#*:}"
    [ -f "${dir}/${1}" ] && readlink -f "${dir}/${1}" && return 0
  done
}

# Oracle Connector
cmd_09="${base_dir}/../009_oracleConnector/Oracle2JSON.py"
cfg_09=`get_config "009.cfg"`
cmd_OC="$cmd_09 --config $cfg_09 --mode SQUASH"

# Stage 25
cmd_25="${base_dir}/../025_chicagoES/stage.py -m s"

# Stage 16
cmd_16="${base_dir}/../016_task2es/task2es.py -m s"

# Stage 19
cfg_19=`get_config "es"`
cmd_19="${base_dir}/../019_esFormat/run.sh --config $cfg_19"

# Stage 69
cfg_69=`get_config "es"`
cmd_69="${base_dir}/../069_upload2es/load_data.sh --config $cfg_69"

# Stage 91
cmd_91="${base_dir}/../091_datasetsRucio/datasets_processing.py -m s"

# Stage 93
cmd_93="${base_dir}/../093_datasetsFormat/datasets_format.py -m s"

# Stage 95
auth_cfg=`get_config "095.cfg"`
source $auth_cfg
[ -r "$AUTH_KEY" -a -r "$AUTH_CRT" ] \
  || { echo "Stage 095: wrong authentication parameters (failed to read certificate files: $AUTH_KEY and $AUTH_CRT)" >&2;
       exit 1; }
cmd_95="${base_dir}/../095_datasetInfoAMI/amiDatasets.py -m s --userkey $AUTH_KEY --usercert $AUTH_CRT"
# ---

# Reset offset value for Oracle Connector.
[ -n "$START_OFFSET" ] \
    && sed -i.bak -e"s/^offset = .*$/offset = $START_OFFSET/" \
           $cfg_09

# Service (glue) functions
# ---
# EOP filter (required due to the unconfigurable EOP marker in pyDKB)
eop_filter() {
  sed -e"s/\\x00//g"
}

# Buffer file name
get_buffer() {
  [ -n "$1" ] && name=$1 || name=default
  echo $buffer_dir/$name
}

# Flush Sink Connector buffer to STDOUT and reset it
flush_buffer() {
  bufer=`get_buffer "$1"`
  if [ -r "$buffer" ]; then
    cat $buffer
    echo -e '\x00'
    rm -f $buffer
  fi
}

# Glue between processing functions and Sink Connector
# Buffers records and then send them as batch of $BATCH_SIZE
mediator() {
  buffer=`get_buffer "$1"`
  i=0
  while read -r line; do
    echo $line >> $buffer
    let i=$i+1
    let f=$i%$BATCH_SIZE
    [ $f -eq 0 ] && flush_buffer "$1"
  done
  flush_buffer "$1"
}
# ---

# Create named pipes for dataflow branching
# ---
branch() {
  [ -z "$1" ] \
    && echo "Branch name not specified." >&2\
    && return 1

  name="$pipe_dir/$1"
  if [ -e "$name" ]; then
    [ -p "$name" ] \
      && { echo $name; return 0; } \
      || name=`mktemp -u $name.XXXX`
  fi

  mkfifo $name \
    || { echo "Failed to create named pipe for branch $1" >&2;
         return 1; }

  echo $name
}

b_91=`branch b_91` && b_16=`branch b_16` \
  || exit $?
# ---

# Run Dataflow
# ---

# Parse command line parameters
while [ -n "$1" ]; do
  case $1 in
    --debug)
      DEBUG=1;;
    --)
      shift
      break;;
    -*)
      echo "Unknown option: $1" >&1;;
    *)
      break;;
  esac
  shift
done


# Define subchains
# Source subchain: Oracle Connector
source_chain() {
  $cmd_OC | tee $b_16 $b_91
}

# Parallel subchains: 16 and 91
chain_16() {
  [ -n DEBUG ] \
    && out_25='25.out'
  cat $b_16 | $cmd_25 | eop_filter | tee $out_25 \
            | $cmd_16 | eop_filter
}

chain_91() {
  out=/dev/null
  [ -n "$DEBUG" ] \
    && out_91='91.out' \
    && out_93='93.out'
  cat $b_91 | $cmd_91 | eop_filter | tee $out_91 \
            | $cmd_93 | eop_filter | tee $out_93 \
            | $cmd_95 | eop_filter
}

# Sink chain
sink_chain() {
  [ -n "$DEBUG" ] \
    && cmd="tee 69.$1.inp" \
    || cmd=$cmd_69
  $cmd_19 | mediator "$1" | $cmd > /dev/null
}

out=/dev/null
[ -n "$DEBUG" ] \
  && out='src.out'
source_chain > $out &

[ -n "$DEBUG" ] \
  && out='16.out'
chain_16 | tee $out | sink_chain b16 &

[ -n "$DEBUG" ] \
  && out='95.out'
chain_91 | tee $out | sink_chain b91 &

ps ax | grep "$cmd_OC" | grep -v grep | awk '{print $1 " source"}' >> $pidfile
for cmd in "$cmd_25" "$cmd_16" "$cmd_91" "$cmd_93" "$cmd_95" "$cmd_19" "$cmd_69"; do
  ps ax | grep "$cmd" | awk '{print $1}' >> $pidfile
done

wait
