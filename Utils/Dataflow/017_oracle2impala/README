=============
* Stage 17  *
=============

1. Description
--------------
Import information from ATLAS ProdSys storage (Oracle DB) to HDFS.

oracle-hdfs-import.sh
---------------------
Uses Sqoop to import data.

USAGE: oracle-hdfs-import.sh [<options>] table_name [table_name...]

OPTIONS:
  Oracle:
    -b, --database     Oracle database code name:
                        DEFT (default)
                        PRODSYS1
    -o, --tab-owner    Oracle table owner. Default:
                        DEFT: ATLAS_DEFT
                        PRODSYS1: ATLAS_GRISLI
    -t, --timestamp    Upper border for timestamp fields.
                       NOTE: the timestamp will be treated in the Oracle local timezone.
    -h, --ora-host     Connection parameter: host (default: 172.17.34.3)
    -p, --ora-port     Connection parameter: port (default: 12344)
    -s, --ora-service  Connection parameter: service (default: adcr.cern.ch)

  HDFS:
    -f, --file-type    Output file type: {text|avro} (default: avro)
    -m, --type-mapping Comma-separated list of maps: <column_name>=<JAVA type>. 
                       No spaces between maps!
                       This custom mapping will be added to the default one.
                       Default mapping is to treat Oracle NUMBER as Integer.
    -O, --output-dir   Output HDFS directory (default: /var/sqoop/USERNAME)
    -n, --no-ts-dir    Do not create timestamp subdirectory in output directory.

  Tables:
    -a, --all-tables   Run script for all known tables.
    -e, --exclude      Comma-separated list of tables to be excluded from operation.
                       No spaces between table names!
    -l, --list-tables  List all the known tables (for given DB) and exit.

2. Before you start
-------------------
Make sure that Oracle is available where you are looking for it.

Bamboo
------
Create SSH tunnel to CERN Oracle server:

  ssh -L 172.17.34.3:12344:itrac5101.cern.ch:10121 USERNAME@lxplus.cern.ch -f sleep 43200

3. TODO
-------
- create Kafka source/sink connectors for this stage 
- add data transformation to get parsed dataset names
- handle Impala table creation over the new data / data append to the existing tables
